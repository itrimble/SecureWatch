services:
  db:
    image: timescale/timescaledb:latest-pg14 # Using latest pg14 version
    container_name: eventlog_db_timescale_local
    ports:
      - '5432:5432' # Expose PostgreSQL port to the host
    volumes:
      - pgdata_eventlog:/var/lib/postgresql/data # Named volume for data persistence
      # To automatically run init scripts, you can mount them to /docker-entrypoint-initdb.d/
      # For example, if create_tables.sql is in the root:
      # - ./create_tables.sql:/docker-entrypoint-initdb.d/01_init_schema.sql
      # Note: Scripts in docker-entrypoint-initdb.d are only run when the database is first initialized (empty pgdata).
    environment:
      POSTGRES_USER: eventlogger # Default user for the database
      POSTGRES_PASSWORD: localdevpassword # Password for the default user
      POSTGRES_DB: eventlog_dev # Name of the default database to create
      # Optional TimescaleDB specific settings if needed at init:
      # TS_TUNE_MEMORY: "4GB" # Example: Adjust based on your system
      # TS_TUNE_NUM_CPUS: "2" # Example: Adjust based on your system
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U eventlogger -d eventlog_dev']
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Kafka for event streaming
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: securewatch_kafka
    ports:
      - '9092:9092'
      - '19092:19092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    restart: unless-stopped

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: securewatch_zookeeper
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: unless-stopped

  # HTTP Event Collector (HEC) Service
  hec-service:
    build:
      context: ./apps/hec-service
      dockerfile: Dockerfile
    container_name: securewatch_hec
    ports:
      - '8888:8888'
    environment:
      - PORT=8888
      - KAFKA_BROKERS=kafka:29092
      - LOG_LEVEL=info
      - RATE_LIMIT_ENABLED=true
      - RATE_LIMIT_WINDOW_MS=60000
      - RATE_LIMIT_MAX_REQUESTS=1000
      - BATCH_SIZE=100
      - BATCH_TIMEOUT_MS=5000
    depends_on:
      - kafka
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8888/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # Log Ingestion Service
  log-ingestion:
    build:
      context: ./apps/log-ingestion
      dockerfile: Dockerfile
    container_name: securewatch_log_ingestion
    ports:
      - '4002:4002'
      # Standard syslog ports
      - '514:514/udp' # UDP syslog
      - '514:514/tcp' # TCP syslog
      - '601:601/tcp' # TCP syslog (RFC 5425)
      - '6514:6514/tcp' # TLS syslog
    environment:
      - PORT=4002
      - KAFKA_BROKERS=kafka:29092
      - CORRELATION_ENGINE_URL=http://correlation-engine:4005
      - SYSLOG_UDP_PORT=514
      - SYSLOG_TCP_PORT=514
      - SYSLOG_RFC5425_PORT=601
      - SYSLOG_TLS_PORT=6514
      - LOG_LEVEL=info
      - DISK_BUFFER_PATH=/var/lib/securewatch/buffers
    volumes:
      - log_buffers:/var/lib/securewatch/buffers
    depends_on:
      - kafka
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:4002/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # Correlation Engine
  correlation-engine:
    build:
      context: ./apps/correlation-engine
      dockerfile: Dockerfile
    container_name: securewatch_correlation
    ports:
      - '4005:4005'
    environment:
      - PORT=4005
      - LOG_LEVEL=info
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:4005/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # Education Service
  education-service:
    build:
      context: .
      dockerfile: ./apps/education-service/Dockerfile
    container_name: securewatch_education
    ports:
      - '4011:4011'
    environment:
      - PORT=4011
      - NODE_ENV=production
      - DB_TYPE=postgres
      - DB_HOST=db
      - DB_PORT=5432
      - DB_USER=eventlogger
      - DB_PASSWORD=localdevpassword
      - DB_NAME=securewatch_education
      - JWT_ACCESS_SECRET=education-service-jwt-secret
      - FRONTEND_URL=http://localhost:4000
      - LOG_LEVEL=info
      - ENROLLMENT_REQUIRED=false
      - ALLOW_GUEST_ACCESS=true
      - ENABLE_CERTIFICATIONS=true
      - ENABLE_LABS=true
      - ENABLE_ASSESSMENTS=true
      - MAX_ENROLLMENTS_PER_USER=20
      - MAX_LAB_DURATION=14400
      - LAB_DEFAULT_ENVIRONMENT=docker
      - LAB_CPU_LIMIT=1
      - LAB_MEMORY_LIMIT=1Gi
      - LAB_STORAGE_LIMIT=10Gi
    depends_on:
      - db
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:4011/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark Master
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: securewatch_spark_master
    ports:
      - '7077:7077'
      - '8080:8080'
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      SPARK_MASTER_OPTS: '-Dspark.deploy.defaultCores=2'
    volumes:
      - spark_data:/opt/bitnami/spark/work-dir
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8080']
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark Worker
  spark-worker:
    image: bitnami/spark:3.5.0
    container_name: securewatch_spark_worker
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2G
      SPARK_WORKER_CORES: 2
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
    volumes:
      - spark_data:/opt/bitnami/spark/work-dir
    depends_on:
      - spark-master
    restart: unless-stopped

  # Spark Batch Processor
  spark-batch-processor:
    build:
      context: ./apps/spark-batch-processor
      dockerfile: Dockerfile
    container_name: securewatch_spark_batch_processor
    ports:
      - '3009:3000'
    environment:
      NODE_ENV: development
      PORT: 3000
      SPARK_MASTER: spark://spark-master:7077
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      PRIMARY_STORAGE: opensearch
      ARCHIVE_STORAGE: s3
      ML_ANOMALY_DETECTION_ENABLED: true
      BATCH_ENABLE_DATA_QUALITY_CHECKS: true
      LOG_LEVEL: info
      SPARK_EXECUTOR_MEMORY: 2g
      SPARK_EXECUTOR_CORES: 2
      SPARK_DRIVER_MEMORY: 1g
    volumes:
      - spark_data:/app/output
      - spark_models:/app/models
      - spark_checkpoint:/tmp/spark-checkpoint
      - ./apps/spark-batch-processor/logs:/app/logs
    depends_on:
      - kafka
      - spark-master
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3000/health']
      interval: 30s
      timeout: 10s
      retries: 3

  # Data Retention Manager
  retention-manager:
    build:
      context: ./apps/retention-manager
      dockerfile: Dockerfile
    container_name: securewatch_retention_manager
    ports:
      - '3012:3012'
    environment:
      NODE_ENV: development
      RETENTION_MANAGER_PORT: 3012
      DATABASE_URL: postgresql://postgres:postgres@postgres-eventlog:5432/eventlog
      REDIS_HOST: redis
      REDIS_PORT: 6379
      LOG_LEVEL: info
      # Storage backends
      AWS_REGION: ${AWS_REGION:-us-west-2}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
      S3_BUCKET: ${S3_BUCKET:-securewatch-archive}
      AZURE_STORAGE_CONNECTION_STRING: ${AZURE_STORAGE_CONNECTION_STRING:-}
      AZURE_CONTAINER: ${AZURE_CONTAINER:-securewatch-archive}
      GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS:-}
      GCS_BUCKET: ${GCS_BUCKET:-securewatch-archive}
      # Alert webhooks
      ALERT_WEBHOOK_URL: ${ALERT_WEBHOOK_URL:-}
      CRITICAL_ALERT_WEBHOOK: ${CRITICAL_ALERT_WEBHOOK:-}
      # Storage limits (GB)
      HOT_STORAGE_LIMIT_GB: 10000
      WARM_STORAGE_LIMIT_GB: 50000
      COLD_STORAGE_LIMIT_GB: 100000
    volumes:
      - ./apps/retention-manager/logs:/app/logs
    depends_on:
      - postgres-eventlog
      - redis
    restart: unless-stopped
    healthcheck:
      test:
        [
          'CMD',
          'wget',
          '--no-verbose',
          '--tries=1',
          '--spider',
          'http://localhost:3012/health',
        ]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  pgdata_eventlog: # Defines the named volume for data persistence
    driver: local
  log_buffers: # Volume for log ingestion buffers
    driver: local
  spark_data: # Volume for Spark data storage
    driver: local
  spark_models: # Volume for ML models
    driver: local
  spark_checkpoint: # Volume for Spark checkpoints
    driver: local
